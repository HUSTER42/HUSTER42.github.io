<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222"><meta name="generator" content="Hexo 7.3.0">

  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/7.0.0/css/all.min.css" integrity="sha256-VHqXKFhhMxcpubYf9xiWdCiojEbY9NexQ4jh8AxbvcM=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/pace/1.2.4/themes/blue/pace-theme-minimal.css">
  <script src="https://cdnjs.cloudflare.com/ajax/libs/pace/1.2.4/pace.min.js" integrity="sha256-gqd7YTjg/BtfqWSwsJOvndl0Bxc8gFImLEkXQT8+qj0=" crossorigin="anonymous" defer></script>

<script class="next-config" data-name="main" type="application/json">{"hostname":"huster42.github.io","root":"/","images":"/images","scheme":"Gemini","darkmode":false,"version":"8.26.0","exturl":false,"sidebar":{"position":"left","width_expanded":320,"width_dual_column":240,"display":"post","padding":18,"offset":12},"hljswrap":true,"codeblock":{"theme":{"light":"default","dark":"stackoverflow-dark"},"prism":{"light":"prism","dark":"prism-dark"},"copy_button":{"enable":true,"style":null},"fold":{"enable":false,"height":500},"language":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":true,"async":true,"duration":200,"transition":{"menu_item":"fadeInDown","post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"i18n":{"placeholder":"搜索...","empty":"没有找到任何搜索结果：${query}","hits_time":"找到 ${hits} 个搜索结果（用时 ${time} 毫秒）","hits":"找到 ${hits} 个搜索结果"}}</script><script src="/js/config.js" defer></script>

    <meta name="description" content="KTF &amp; nano vllm 一、KTransformers（简称 KTF）1.1 项目简介KTransformers 是由清华大学 KVCache.AI 团队联合趋境科技推出的开源项目，核心目标是优化大语言模型推理性能、降低硬件门槛，尤其适配资源受限场景。优化的初衷是GPU算力存储的成本过高，希望能够使用内存，硬盘等成本低容量大带宽低的设备进行代替，并且更加符合稀疏矩阵的场景。">
<meta property="og:type" content="article">
<meta property="og:title" content="KTF初探">
<meta property="og:url" content="http://huster42.github.io/2025/11/29/KTF%E5%88%9D%E6%8E%A2/index.html">
<meta property="og:site_name" content="HUSTER42&#39;s BLOG">
<meta property="og:description" content="KTF &amp; nano vllm 一、KTransformers（简称 KTF）1.1 项目简介KTransformers 是由清华大学 KVCache.AI 团队联合趋境科技推出的开源项目，核心目标是优化大语言模型推理性能、降低硬件门槛，尤其适配资源受限场景。优化的初衷是GPU算力存储的成本过高，希望能够使用内存，硬盘等成本低容量大带宽低的设备进行代替，并且更加符合稀疏矩阵的场景。">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="c:\Users\xsjch\AppData\Roaming\Typora\typora-user-images\image-20251101204047092.png">
<meta property="og:image" content="c:\Users\xsjch\AppData\Roaming\Typora\typora-user-images\image-20251030082436022.png">
<meta property="og:image" content="c:\Users\xsjch\AppData\Roaming\Typora\typora-user-images\image-20251030083525979.png">
<meta property="article:published_time" content="2025-11-29T12:49:21.000Z">
<meta property="article:modified_time" content="2025-11-29T13:22:25.776Z">
<meta property="article:author" content="HUSTER42">
<meta property="article:tag" content="KTF">
<meta property="article:tag" content="深度学习">
<meta property="article:tag" content="模型部署">
<meta property="article:tag" content="推理加速">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="c:\Users\xsjch\AppData\Roaming\Typora\typora-user-images\image-20251101204047092.png">


<link rel="canonical" href="http://huster42.github.io/2025/11/29/KTF%E5%88%9D%E6%8E%A2/">


<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":false,"isPost":true,"lang":"zh-CN","comments":true,"permalink":"http://huster42.github.io/2025/11/29/KTF%E5%88%9D%E6%8E%A2/","path":"2025/11/29/KTF初探/","title":"KTF初探"}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>KTF初探 | HUSTER42's BLOG</title>
  








  
  <script size="300" alpha="0.6" zIndex="-1" src="https://cdnjs.cloudflare.com/ajax/libs/ribbon.js/1.0.2/ribbon.min.js" defer></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous" defer></script>
<script src="/js/utils.js" defer></script><script src="/js/motion.js" defer></script><script src="/js/sidebar.js" defer></script><script src="/js/next-boot.js" defer></script>

  




  <script src="/js/third-party/pace.js" defer></script>


  
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>




  

  <script class="next-config" data-name="enableMath" type="application/json">true</script><script class="next-config" data-name="mathjax" type="application/json">{"enable":true,"tags":"none","js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.js","integrity":"sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI="}}</script>
<script src="/js/third-party/math/mathjax.js" defer></script>



  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <div class="column">
      <header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <p class="site-title">HUSTER42's BLOG</p>
      <i class="logo-line"></i>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger" aria-label="搜索" role="button">
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a></li>
  </ul>
</nav>




</header>
        
  
  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#KTF-nano-vllm"><span class="nav-number">1.</span> <span class="nav-text">KTF &amp; nano vllm</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%B8%80%E3%80%81KTransformers%EF%BC%88%E7%AE%80%E7%A7%B0-KTF%EF%BC%89"><span class="nav-number">1.1.</span> <span class="nav-text">一、KTransformers（简称 KTF）</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#1-1-%E9%A1%B9%E7%9B%AE%E7%AE%80%E4%BB%8B"><span class="nav-number">1.1.0.1.</span> <span class="nav-text">1.1 项目简介</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#1-2-%E4%B8%BA%E4%BB%80%E4%B9%88%E6%95%88%E6%9E%9C%E5%A5%BD%EF%BC%9F"><span class="nav-number">1.1.0.2.</span> <span class="nav-text">1.2 为什么效果好？</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#1-3-%E5%AE%9A%E4%BD%8D%E5%B7%AE%E5%BC%82"><span class="nav-number">1.1.0.3.</span> <span class="nav-text">1.3 定位差异</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#1-4-%E5%8F%82%E8%80%83%E8%B5%84%E6%BA%90"><span class="nav-number">1.1.0.4.</span> <span class="nav-text">1.4 参考资源</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%BA%8C%E3%80%81nano-vllm-%E9%83%A8%E7%BD%B2%E5%AE%9E%E8%B7%B5"><span class="nav-number">1.2.</span> <span class="nav-text">二、nano vllm 部署实践</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#2-1-%E9%83%A8%E7%BD%B2%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE"><span class="nav-number">1.2.0.1.</span> <span class="nav-text">2.1 部署环境配置</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-2-%E9%83%A8%E7%BD%B2%E8%BF%87%E7%A8%8B%E4%B8%AD%E7%9A%84%E9%97%AE%E9%A2%98"><span class="nav-number">1.2.0.2.</span> <span class="nav-text">2.2 部署过程中的问题</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-2-1-triton-%E5%8C%85%E6%94%AF%E6%8C%81%E9%97%AE%E9%A2%98"><span class="nav-number">1.2.0.3.</span> <span class="nav-text">2.2.1 triton 包支持问题</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-2-2-flash-attn-%E5%BA%93%E5%AE%89%E8%A3%85%E6%8A%A5%E9%94%99"><span class="nav-number">1.2.0.4.</span> <span class="nav-text">2.2.2 flash-attn 库安装报错</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-2-3-NCCL%E6%8A%A5%E9%94%99"><span class="nav-number">1.2.0.5.</span> <span class="nav-text">2.2.3 NCCL报错</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-3-FlashAttention-%E7%AE%97%E5%AD%90%E8%AF%A6%E8%A7%A3"><span class="nav-number">1.2.0.6.</span> <span class="nav-text">2.3 FlashAttention 算子详解</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-4-%E6%88%90%E5%8A%9F%E9%83%A8%E7%BD%B2"><span class="nav-number">1.2.0.7.</span> <span class="nav-text">2.4 成功部署</span></a></li></ol></li></ol></li></ol></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="HUSTER42"
      src="/images/science.jpeg">
  <p class="site-author-name" itemprop="name">HUSTER42</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">4</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
          <a href="/categories/">
        <span class="site-state-item-count">1</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="/tags/">
        <span class="site-state-item-count">8</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author animated">
      <span class="links-of-author-item">
        <a href="https://github.com/HUSTER42" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;HUSTER42" rel="noopener me" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
  </div>

        </div>
      </div>
    </div>

    
    <div class="sidebar-inner sidebar-blogroll">
      <div class="links-of-blogroll animated">
        <div class="links-of-blogroll-title"><i class="fa fa-globe fa-fw"></i>
          链接
        </div>
        <ul class="links-of-blogroll-list">
            <li class="links-of-blogroll-item">
              <a href="https://example.com/" title="https:&#x2F;&#x2F;example.com" rel="noopener" target="_blank">Title</a>
            </li>
        </ul>
      </div>
    </div>
  </aside>


    </div>

    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://huster42.github.io/2025/11/29/KTF%E5%88%9D%E6%8E%A2/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/science.jpeg">
      <meta itemprop="name" content="HUSTER42">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="HUSTER42's BLOG">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="KTF初探 | HUSTER42's BLOG">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          KTF初探
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>
      

      <time title="创建时间：2025-11-29 20:49:21 / 修改时间：21:22:25" itemprop="dateCreated datePublished" datetime="2025-11-29T20:49:21+08:00">2025-11-29</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/KTF/" itemprop="url" rel="index"><span itemprop="name">KTF</span></a>
        </span>
    </span>

  
    <span class="post-meta-item" title="阅读次数" id="busuanzi_container_page_pv">
      <span class="post-meta-item-icon">
        <i class="far fa-eye"></i>
      </span>
      <span class="post-meta-item-text">阅读次数：</span>
      <span id="busuanzi_value_page_pv"></span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody"><h1 id="KTF-nano-vllm"><a href="#KTF-nano-vllm" class="headerlink" title="KTF &amp; nano vllm"></a>KTF &amp; nano vllm</h1><hr>
<h2 id="一、KTransformers（简称-KTF）"><a href="#一、KTransformers（简称-KTF）" class="headerlink" title="一、KTransformers（简称 KTF）"></a>一、KTransformers（简称 KTF）</h2><h4 id="1-1-项目简介"><a href="#1-1-项目简介" class="headerlink" title="1.1 项目简介"></a>1.1 项目简介</h4><p>KTransformers 是由清华大学 KVCache.AI 团队联合趋境科技推出的开源项目，核心目标是<strong>优化大语言模型推理性能、降低硬件门槛</strong>，尤其适配资源受限场景。优化的初衷是GPU算力存储的成本过高，希望能够使用内存，硬盘等成本低容量大带宽低的设备进行代替，并且更加符合稀疏矩阵的场景。</p>
<span id="more"></span>
<h4 id="1-2-为什么效果好？"><a href="#1-2-为什么效果好？" class="headerlink" title="1.2 为什么效果好？"></a>1.2 为什么效果好？</h4><p>基于底层技术优化，KTF 在低显存环境下实现高性能推理，关键特性包括：</p>
<ul>
<li>以存代算：假如前缀相同就可以存储中间结果回答相似问题（K&#x2F;Vcache）。</li>
<li>MoE:切分稠密的FFE矩阵成小块，每次只激活部分稀疏矩阵。可以考虑更多的硬件选择</li>
<li>基于计算强度的Offload策略：针对算子的计算强度优先级丢到CPU上，不让GPU参与</li>
<li>利用CudaGraph（把一系列的一串的kernel打包成一个CudaGraph）提高解码速度，降低CPU&#x2F;GPU交互延迟(CPU在跑矩阵乘法每下达一个Kernel指令，都需要与GPU进行一次交互，这会产生微小的延迟)</li>
<li>AMX指令在CPU(负责专家计算）上用于矩阵计算，利用内存和指令排布来实现理论性能（类似通用矩阵乘GEMM优化）<ul>
<li><strong>传统CPU指令</strong>：一次操作只能处理几个数据，这被称为SIMD（单指令多数据流）。</li>
<li><strong>AMX指令</strong>： 它引入了一个叫 <strong>“TILE”</strong> 的二维寄存器文件，可以看作是一大块专门用于矩阵运算的片上缓存。一条AMX指令可以直接对一个<code>TILE x TILE</code>大小的矩阵块进行操作，而不是几个标量。</li>
</ul>
</li>
</ul>
<h4 id="1-3-定位差异"><a href="#1-3-定位差异" class="headerlink" title="1.3 定位差异"></a>1.3 定位差异</h4><table>
<thead>
<tr>
<th>工具</th>
<th>核心侧重</th>
<th>适用场景</th>
</tr>
</thead>
<tbody><tr>
<td>vLLM</td>
<td>大规模部署</td>
<td>高资源、高并发场景</td>
</tr>
<tr>
<td>KTransformers</td>
<td>资源受限环境的本地推理</td>
<td>个人开发者、小算力场景</td>
</tr>
</tbody></table>
<h4 id="1-4-参考资源"><a href="#1-4-参考资源" class="headerlink" title="1.4 参考资源"></a>1.4 参考资源</h4><p>感谢 B 站官方提供的部署教程与技术解析：</p>
<ul>
<li><a target="_blank" rel="noopener" href="https://www.bilibili.com/video/BV1VNQrYGEad/?spm_id_from=333.337.search-card.all.click">KTransformers 团队分享异构推理架构思路</a></li>
<li><a target="_blank" rel="noopener" href="https://www.bilibili.com/video/BV17dQ6YEEzn/?spm_id_from=333.337.search-card.all.click">KTransformers 开发团队手把手教部署</a></li>
</ul>
<p>Windows 环境暂不支持 KTF 安装，因此采用 <strong>MobaXterm（v25.3）</strong> 作为终端工具，连接服务器进行后续操作。</p>
<p>目前的服务器环境如下：</p>
<table>
<thead>
<tr>
<th>硬件</th>
<th>版本信息</th>
<th>其他信息</th>
</tr>
</thead>
<tbody><tr>
<td>CPU</td>
<td>Intel(R) Xeon(R) Gold 5218 CPU</td>
<td>x86_64架构，总计<strong>32 核 64 线程</strong></td>
</tr>
<tr>
<td>GPU</td>
<td>NVIDIA A100-PCIE-40GB</td>
<td>CUDA 版本<code>13.0</code></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
</tr>
</tbody></table>
<hr>
<h2 id="二、nano-vllm-部署实践"><a href="#二、nano-vllm-部署实践" class="headerlink" title="二、nano vllm 部署实践"></a>二、nano vllm 部署实践</h2><h4 id="2-1-部署环境配置"><a href="#2-1-部署环境配置" class="headerlink" title="2.1 部署环境配置"></a>2.1 部署环境配置</h4><p>因 Windows 环境存在依赖兼容性问题，需调整环境，最终配置如下：</p>
<table>
<thead>
<tr>
<th>环境组件</th>
<th>版本信息</th>
</tr>
</thead>
<tbody><tr>
<td>Python</td>
<td>3.10</td>
</tr>
<tr>
<td>PyTorch</td>
<td>2.8.0 + cu128</td>
</tr>
<tr>
<td>终端工具</td>
<td>Windows 本地</td>
</tr>
</tbody></table>
<h4 id="2-2-部署过程中的问题"><a href="#2-2-部署过程中的问题" class="headerlink" title="2.2 部署过程中的问题"></a>2.2 部署过程中的问题</h4><h4 id="2-2-1-triton-包支持问题"><a href="#2-2-1-triton-包支持问题" class="headerlink" title="2.2.1 triton 包支持问题"></a>2.2.1 triton 包支持问题</h4><ul>
<li><p>问题：Windows 环境不支持直接下载官方 triton 包</p>
</li>
<li><p>解决：采用triton-windows替代，但需 Python 3.10+ 版本，因此需更新虚拟环境至 Python 3.10，并且重新安装适配的 PyTorch 包（确保与 cu128 兼容）最终采用下列指令成功下载：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install -U <span class="string">&quot;triton-windows&lt;3.5&quot;</span></span><br></pre></td></tr></table></figure></li>
</ul>
<h4 id="2-2-2-flash-attn-库安装报错"><a href="#2-2-2-flash-attn-库安装报错" class="headerlink" title="2.2.2 flash-attn 库安装报错"></a>2.2.2 flash-attn 库安装报错</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install flash-attn</span><br></pre></td></tr></table></figure>

<p>失败，报错信息如下：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ERROR: Could not install packages due to an OSError: [Errno 2] No such file or directory: <span class="string">&#x27;C:\\Users\\xsjch\\AppData\\Local\\Temp\\pip-install-tf_qh1gh\\flash-attn_3f99701029314213b0966f61917caf3a\\csrc\\composable_kernel\\library\\include\\ck\\library\\tensor_operation_instance\\gpu\\grouped_conv_bwd_weight\\device_grouped_conv_bwd_weight_two_stage_xdl_instance.hpp&#x27;</span></span><br></pre></td></tr></table></figure>

<ul>
<li><p>原因：官方未提供 Windows 版本的 flash-attn 库</p>
</li>
<li><p>解决：使用社区适配的 Windows 版本，下载链接如下：<a target="_blank" rel="noopener" href="https://github.com/kingbri1/flash-attention/releases/download/v2.8.3/flash_attn-2.8.3+cu128torch2.8.0cxx11abiFALSE-cp310-cp310-win_amd64.whl">https://github.com/kingbri1/flash-attention/releases/download/v2.8.3/flash_attn-2.8.3+cu128torch2.8.0cxx11abiFALSE-cp310-cp310-win_amd64.whl</a></p>
</li>
</ul>
<h4 id="2-2-3-NCCL报错"><a href="#2-2-3-NCCL报错" class="headerlink" title="2.2.3 NCCL报错"></a>2.2.3 NCCL报错</h4><p>NCCL 是 NVIDIA 提供的用于实现 GPU 加速的通信库，目前主要用于分布式训练。然而，NCCL 并不支持在 Windows 操作系统上运行，因此需要使用其它的通信库来替代 NCCL。在 PyTorch 中，可以将分布式后端设置为 “gloo”，来使用 Gloo 通信库来替代 NCCL。于是我将nano-vllm-main\nanovllm\engine\model_runner.py中的nccl改为gloo。</p>
<p>但我同时意识到没有多个GPU去做多GPU之间的通信，所以采用了单GPU的模式进行运算。</p>
<h4 id="2-3-FlashAttention-算子详解"><a href="#2-3-FlashAttention-算子详解" class="headerlink" title="2.3 FlashAttention 算子详解"></a>2.3 FlashAttention 算子详解</h4><p>顺便在下载过程中了解flash-attn算子：<a href="https://link.zhihu.com/?target=https://arxiv.org/abs/2205.14135">https://link.zhihu.com/?target=https%3A//arxiv.org/abs/2205.14135</a></p>
<p>FlashAttention是一种加速注意力计算方法，由于Transformer中self-attention 的时间和内存复杂度是序列长度的<strong>二次方</strong>，所以序列过长时，算法速度会变慢，需要消耗很高的内存。</p>
<p><strong>核心思想</strong>：减少HBM的访问，将QKV切分为小块后放入SRAM中，通过融合注意力计算中的多个操作（如 Q, K, V 矩阵乘法、softmax 归一化、与 V 的加权求和等），直接在 GPU的SRAM中完成计算。</p>
<p><img src="C:\Users\xsjch\AppData\Roaming\Typora\typora-user-images\image-20251101204047092.png" alt="image-20251101204047092"></p>
<h4 id="2-4-成功部署"><a href="#2-4-成功部署" class="headerlink" title="2.4 成功部署"></a>2.4 成功部署</h4><p><img src="C:\Users\xsjch\AppData\Roaming\Typora\typora-user-images\image-20251030082436022.png" alt="image-20251030082436022"></p>
<p>但是仍然有一处警告（因为在windows上跑）</p>
<p><img src="C:\Users\xsjch\AppData\Roaming\Typora\typora-user-images\image-20251030083525979.png" alt="image-20251030083525979"></p>
<p><code>record_context_cpp</code> 是用于记录和调试上下文信息的一个工具，通常用在性能分析或错误跟踪中，与推理核心功能并无影响，我也没能找到替代方案。</p>

    </div>

    
    
    

    <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/KTF/" rel="tag"># KTF</a>
              <a href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" rel="tag"># 深度学习</a>
              <a href="/tags/%E6%A8%A1%E5%9E%8B%E9%83%A8%E7%BD%B2/" rel="tag"># 模型部署</a>
              <a href="/tags/%E6%8E%A8%E7%90%86%E5%8A%A0%E9%80%9F/" rel="tag"># 推理加速</a>
          </div>

        

          <div class="post-nav">
            <div class="post-nav-item">
            </div>
            <div class="post-nav-item">
                <a href="/2025/11/29/KTF%E9%83%A8%E7%BD%B2/" rel="next" title="KTF部署">
                  KTF部署 <i class="fa fa-angle-right"></i>
                </a>
            </div>
          </div>
    </footer>
  </article>
</div>






</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">

  <div class="copyright">
    &copy; 
    <span itemprop="copyrightYear">2025</span>
    <span class="with-love">
      <i class="fa fa-heart"></i>
    </span>
    <span class="author" itemprop="copyrightHolder">HUSTER42</span>
  </div>
<div class="busuanzi-count">
    <span class="post-meta-item" id="busuanzi_container_site_uv">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="总访客量">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-item" id="busuanzi_container_site_pv">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="总访问量">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>

    </div>
  </footer>

  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>
  <div class="sidebar-dimmer"></div>
  <div class="back-to-top" role="button" aria-label="返回顶部">
    <i class="fa fa-arrow-up fa-lg"></i>
    <span>0%</span>
  </div>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>

</body>
</html>
